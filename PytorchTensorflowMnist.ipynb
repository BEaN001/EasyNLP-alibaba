{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a PyTorch model to Tensorflow using ONNX\n",
    "\n",
    "In this tutorial, we will show you how to export a model defined in PyTorch to ONNX and then import the ONNX model into Tensorflow to run it. We will also show you how to save this Tensorflow model into a file for later use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations\n",
    "\n",
    "First let's install [ONNX](https://github.com/onnx/onnx), [PyTorch](https://github.com/pytorch/pytorch), and [Tensorflow](https://github.com/tensorflow/tensorflow) by following the instructions on each of their repository.\n",
    "\n",
    "Then Install torchvision by the following command:\n",
    "```\n",
    "pip install torchvision\n",
    "```\n",
    "\n",
    "Next install [onnx-tensorflow](https://github.com/onnx/onnx-tensorflow) by the following commands:\n",
    "```\n",
    "git clone git@github.com:onnx/onnx-tensorflow.git && cd onnx-tensorflow\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "\n",
    "In this tutorial we are going to use the [MNIST model](https://github.com/pytorch/examples/tree/master/mnist) from PyTorch examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xdrlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        # x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        # x = x.view(-1, 320)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.fc2(x)\n",
    "        return [self.func(x), self.funcy(y), self.funcz(z)]\n",
    "        # if y and not z:\n",
    "        #     return [F.log_softmax(x, dim=1), self.generate(y), None]\n",
    "        # if not y and z:\n",
    "        #     return [F.log_softmax(x, dim=1), None, self.decode(z)]\n",
    "        # if y and z:\n",
    "        #     return [F.log_softmax(x, dim=1), self.generate(y), self.decode(z)]\n",
    "        # return [F.log_softmax(x, dim=1), None, None]\n",
    "        # return [F.log_softmax(x, dim=1), self.generate(y), self.decode(z)]\n",
    "    \n",
    "    def func(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def funcy(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def funcz(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def generate(self, x):\n",
    "        return 'generate functionÔºÅ'\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return 'decode function!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model\n",
    "Now let's train this model. By default if GPU is availalbe on your environment, it will use GPU instead of CPU to run the training. In this tutorial we will train this model with 60 epochs. It will takes about 15 minutes on an environment with 1 GPU to complete this training. You can always adjust the number of epoch base on how well you want your model to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.327472\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.797767\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.707950\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.618783\n",
      "\n",
      "Test set: Average loss: 0.1994, Accuracy: 9411/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, data, data)[0]\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data, data, data)[0]\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "# Train this model with 60 epochs and after process every 300 batches log the train status \n",
    "args = parser.parse_args(['--epochs', '1', '--log-interval', '300'])\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "torch.save(model.state_dict(), 'output/mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the trained model to ONNX \n",
    "In order to export the model, Pytorch exporter needs to run the model once and save this resulting traced model to a ONNX file. Therefore, we need to provide the input for our MNIST model. Here we would expect to get a black and white 28 x 28 picture as an input to run this model in inference phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-2.6808, -2.6145, -2.0503, -2.4332, -3.0650, -1.7016, -2.5689, -3.1649,\n",
      "         -1.4427, -2.8532]], grad_fn=<LogSoftmaxBackward>), tensor([[ 0.2808,  0.1121,  0.5388, -0.4062,  0.0297, -0.3692,  0.1059, -0.2247,\n",
      "          0.0535, -0.2588]], grad_fn=<AddmmBackward>), tensor([[-0.9936, -0.0490,  0.3398,  1.2832,  0.1700,  0.3434, -1.1712,  0.2749,\n",
      "          0.1597,  0.6348]], grad_fn=<AddmmBackward>)]\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Load the trained model from file\n",
    "trained_model = Net()\n",
    "trained_model.load_state_dict(torch.load('output/mnist.pth'))\n",
    "\n",
    "# Export the trained model to ONNX\n",
    "dummy_input1 = Variable(torch.randn(1, 1, 28, 28)) # one black and white 28 x 28 picture will be the input to the model\n",
    "dummy_input2 = Variable(torch.randn(1, 1, 28, 28)) # one black and white 28 x 28 picture will be the input to the model\n",
    "dummy_input3 = Variable(torch.randn(1, 1, 28, 28)) # one black and white 28 x 28 picture will be the input to the model\n",
    "\n",
    "example_output = trained_model(dummy_input1, dummy_input2, dummy_input3)\n",
    "print(example_output)\n",
    "\n",
    "torch.onnx.export(trained_model, \n",
    "                (dummy_input1, dummy_input2, dummy_input3),\n",
    "                \"output/mnist.onnx\", \n",
    "                input_names=[\"x\", \"y\", \"z\"],\n",
    "                output_names=[\"output1\", \"output2\", \"output3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS. You can examine the graph of this mnist.onnx file using an ONNX viewer call [Netron](https://github.com/lutzroeder/Netron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the ONNX model to Tensorflow\n",
    "We will use onnx_tf.backend.prepare to import the ONNX model into Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# Load the ONNX file\n",
    "model = onnx.load('output/mnist.onnx')\n",
    "\n",
    "# Import the ONNX model to Tensorflow\n",
    "tf_rep = prepare(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the tf_rep object return from onnx.tf.backend.prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: ['x', 'y', 'z']\n",
      "outputs: ['output1', 'output2', 'output3']\n",
      "tensor_dict:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Input nodes to the model\n",
    "print('inputs:', tf_rep.inputs)\n",
    "\n",
    "# Output nodes from the model\n",
    "print('outputs:', tf_rep.outputs)\n",
    "\n",
    "# All nodes in the model\n",
    "print('tensor_dict:')\n",
    "print(tf_rep.tensor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABQGlDQ1BJQ0MgUHJvZmlsZQAAeJxjYGDiSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8bAwcDDwMXAyMCbmFxc4BgQ4ANUwgCjUcG3a0B1QHBZF2TW97/mRpNnhby6vnvb+smik19iqkcBXCmpxclA+g8QxycXFJUwMDDGANnK5SUFIHYDkC1SBHQUkD0FxE6HsFeA2EkQ9h6wmpAgZyD7ApAtkJyRmAJkPwCydZKQxNOR2FB7QYDVyNzJkIA7SQYlqRUlINo5v6CyKDM9o0TBERg6qQqeecl6OgpGBkZGDAygsIao/iwGDkNGsVMIsaJVDAzW9QwMTN8QYtHPGBjW3GBgENyPENNwY2AQB4bHvvKCxKJEuAMYv7EUpxkbQdg8RUBv/vj//7MsAwP7LgaGv0X///+e+///3yUMDMw3GRgOFAIAzSJgR8ipDLsAAAA4ZVhJZk1NACoAAAAIAAGHaQAEAAAAAQAAABoAAAAAAAKgAgAEAAAAAQAAAzWgAwAEAAAAAQAAAygAAAAAMNPRNQAAAqhJREFUeJxt01tI02EYx/Hf++4/l5vztGlqYliGZ5dJ6mZaYVB0QBFSjAy8CLux1IiUots0KPUmFUFLI5MososM0bJQSb1QxNrSUufQxDab87C5/+HtQhE7PJfP5+a5+D6EYXNMb4bX/VRYsfulnQ7dXDnJJpqbTbr0CLUMgt34YVJfoNmBHS3p+RrH6oYImadaZWmeuKbfxrr35boFu0gAgIHTarsfFmYBTg5AY3+teoInFABAIC7Yj4XcVpwCKPDubZVyWqBk6zIQujEZcafOBFAs1Zf5z2KbAIBIU7rcGp5QtEUazH8aQARzrtCtoLa+S1b3XwbQVVd+xzr9GLTP+o8BZFHvGqP9qS7+P4h1efQANUevAgAYY5IkSRJjjDFJAt2IM3J8gJMxMCLj5BwlTOQFgezyUnoQEmzj5J4q5vRQKanEGEAIVRJxdXZyyhJz0s1BVHvzcp6uzC/aXSCBB1ZGBr7aRIhaAo53PjunYMoRlz7GuTAXKLT1LxFKZXBr1hTUY9lxC+bxRKmkFeHJEZ+6bHK5jAAIt2hp+GfD96rY5d6UkvaiQWkm51EW5QEwRdxYLE3vTwgaajVs9ATcWyxvC562lT6Id0sQI/2/pNEjVusZ8vxbLN+nrUBTTYzYGVR92UNgeYNesdT3eFNeKN8Q6iP2HTpPOivjQ7qNBffDkpKf5CgIcxRd8bnhqogaIv6HS2ak7LKBqdBUF16OVvMU3sXVmptcu0ZJrT8KIX/1Qq+19PkaX5fKGAUMudejqpZH94qcMT5B5BpNSbtTByvLwgEK4MLZq1KDKkgSXTPZorDyNCCz625xBoDNNHvqEy+GWeYdOFirTomfe2wt0+2IerFleH9GlK9M5v413jt/Is9rZ/GApWvgp6eSrbn3HM3Ubr3DbzjKKjGaOOhpAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6CAAE17F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs(output1=array([[-124.25574 ,  -87.067764,  -88.25682 ,  -64.05246 ,  -98.95877 ,\n",
      "         -46.48574 , -126.64212 , -113.50561 ,    0.      ,  -72.48548 ]],\n",
      "      dtype=float32), output2=array([[-36.966267  ,   0.22170016,  -0.96735525,  23.237011  ,\n",
      "        -11.669303  ,  40.803726  , -39.35265   , -26.216139  ,\n",
      "         87.28947   ,  14.803986  ]], dtype=float32), output3=array([[-36.966267  ,   0.22170016,  -0.96735525,  23.237011  ,\n",
      "        -11.669303  ,  40.803726  , -39.35265   , -26.216139  ,\n",
      "         87.28947   ,  14.803986  ]], dtype=float32))\n",
      "[[-124.25574   -87.067764  -88.25682   -64.05246   -98.95877   -46.48574\n",
      "  -126.64212  -113.50561     0.        -72.48548 ]]\n",
      "[[-36.966267     0.22170016  -0.96735525  23.237011   -11.669303\n",
      "   40.803726   -39.35265    -26.216139    87.28947     14.803986  ]]\n",
      "[[-36.966267     0.22170016  -0.96735525  23.237011   -11.669303\n",
      "   40.803726   -39.35265    -26.216139    87.28947     14.803986  ]]\n",
      "The digit is classified as  18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "print('Image 1:')\n",
    "img = Image.open('/home/yubin/EasyNLP-alibaba/20220812102730.jpg').resize((28, 28)).convert('L')\n",
    "display(img)\n",
    "output = tf_rep.run((np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :], np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :], np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :]))\n",
    "print(output)\n",
    "print(output['output1'])\n",
    "print(output['output2'])\n",
    "print(output['output3'])\n",
    "print('The digit is classified as ', np.argmax(output))\n",
    "\n",
    "# print('Image 2:')\n",
    "# img = Image.open('assets/three.png').resize((28, 28)).convert('L')\n",
    "# display(img)\n",
    "# output = tf_rep.run(np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :])\n",
    "# print('The digit is classified as ', np.argmax(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Tensorflow model into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 10:18:52.812814: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/mnist.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/mnist.pb/assets\n"
     ]
    }
   ],
   "source": [
    "tf_rep.export_graph('output/mnist.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.8.5 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "59c0781f99bb873beca84764973919837529f603c5934888e3f18bbeeda74045"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
