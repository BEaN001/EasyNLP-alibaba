{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'easy_predict'\n"
     ]
    }
   ],
   "source": [
    "from easynlp.utils import get_pretrain_model_path\n",
    "from easynlp.appzoo import TextImageGenerationPredictor\n",
    "from easynlp.core import PredictorManager\n",
    "from easynlp.appzoo import TextImageGeneration\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Restored from /home/yubin/EasyNLP-alibaba/tmp/finetune_model_MUGE\n"
     ]
    }
   ],
   "source": [
    "user_defined_parameters = {'max_generated_num': 1}\n",
    "pretrained_model_name_or_path = get_pretrain_model_path('/home/yubin/EasyNLP-alibaba/tmp/finetune_model_MUGE')\n",
    "predictor = TextImageGenerationPredictor(model_dir=pretrained_model_name_or_path, model_cls=TextImageGeneration,\n",
    "                                        user_defined_parameters=user_defined_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idx': 0, 'first_sequence': '黄色秋冬针织衫童装儿童内搭上衣'},\n",
       " {'idx': 1, 'first_sequence': '绿色秋冬针织衫童装儿童内搭上衣'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"黄色秋冬针织衫童装儿童内搭上衣\", \"绿色秋冬针织衫童装儿童内搭上衣\"]\n",
    "inputs = [{'idx': idx, 'first_sequence': data[idx]} for idx in range(len(data))]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': [0, 1],\n",
       " 'input_ids': [array([24326, 22066, 21288, 17484, 23535, 21686, 22520, 21381, 22547,\n",
       "         17420, 21381, 17463, 19406, 17061, 22516, 16384, 16384, 16384,\n",
       "         16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384,\n",
       "         16384, 16384, 16384, 16384, 16384]),\n",
       "  array([21728, 22066, 21288, 17484, 23535, 21686, 22520, 21381, 22547,\n",
       "         17420, 21381, 17463, 19406, 17061, 22516, 16384, 16384, 16384,\n",
       "         16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384,\n",
       "         16384, 16384, 16384, 16384, 16384])]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = predictor.preprocess(inputs)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24326, 22066, 21288, 17484, 23535, 21686, 22520, 21381, 22547, 17420,\n",
      "         21381, 17463, 19406, 17061, 22516, 16384, 16384, 16384, 16384, 16384,\n",
      "         16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384,\n",
      "         16384, 16384],\n",
      "        [21728, 22066, 21288, 17484, 23535, 21686, 22520, 21381, 22547, 17420,\n",
      "         21381, 17463, 19406, 17061, 22516, 16384, 16384, 16384, 16384, 16384,\n",
      "         16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384,\n",
      "         16384, 16384]], device='cuda:0') tensor([  2, 256,  16,  16])\n",
      "tensor([[24326, 22066, 21288, 17484, 23535, 21686, 22520, 21381, 22547, 17420,\n",
      "         21381, 17463, 19406, 17061, 22516, 16384, 16384, 16384, 16384, 16384,\n",
      "         16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384,\n",
      "         16384, 16384],\n",
      "        [21728, 22066, 21288, 17484, 23535, 21686, 22520, 21381, 22547, 17420,\n",
      "         21381, 17463, 19406, 17061, 22516, 16384, 16384, 16384, 16384, 16384,\n",
      "         16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384,\n",
      "         16384, 16384]], device='cuda:0') tensor([  2, 256,  16,  16])\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict_postprocess(model_inputs)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[0]['gen_imgbase64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input1 = torch.tensor([[24326, 22066, 21288, 17484, 23535, 21686, 22520, 21381, 22547, 17420,\n",
    "#          21381, 17463, 19406, 17061, 22516, 16384, 16384, 16384, 16384, 16384,\n",
    "#          16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384,\n",
    "#          16384, 16384],\n",
    "#         [21728, 22066, 21288, 17484, 23535, 21686, 22520, 21381, 22547, 17420,\n",
    "#          21381, 17463, 19406, 17061, 22516, 16384, 16384, 16384, 16384, 16384,\n",
    "#          16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384,\n",
    "#          16384, 16384]], device='cuda:0') \n",
    "# input1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input2 = torch.tensor([  2, 256,  16,  16])\n",
    "# input2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = predictor.postprocess(model_outputs)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 黄色秋冬针织衫童装儿童内搭上衣, save generated image: ['黄色秋冬针织衫童装儿童内搭上衣_27.png']\n",
      "text: 绿色秋冬针织衫童装儿童内搭上衣, save generated image: ['绿色秋冬针织衫童装儿童内搭上衣_14.png']\n"
     ]
    }
   ],
   "source": [
    "def base64_to_image(imgbase64_str):\n",
    "    image = Image.open(BytesIO(base64.urlsafe_b64decode(imgbase64_str)))\n",
    "    return image\n",
    "for text, result in zip(data, results):\n",
    "    imgbase64_str_list = result['gen_imgbase64']\n",
    "    imgpath_list = []\n",
    "    for base64_idx in range(len(imgbase64_str_list)):\n",
    "        imgbase64_str = imgbase64_str_list[base64_idx]\n",
    "        image = base64_to_image(imgbase64_str)\n",
    "        imgpath = '{}_{}.png'.format(text, base64_idx)\n",
    "        import os\n",
    "        while os.path.exists(imgpath):\n",
    "            base64_idx += 1\n",
    "            imgpath = '{}_{}.png'.format(text, base64_idx)\n",
    "        image.save(imgpath)\n",
    "        imgpath_list.append(imgpath)\n",
    "    print ('text: {}, save generated image: {}'.format(text, imgpath_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yubin/EasyNLP-alibaba/easynlp/modelzoo/models/artist/modeling_artist.py:145: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
      "/home/yubin/miniconda3/envs/torch/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /tmp/pip-req-build-ex__3qls/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "/home/yubin/EasyNLP-alibaba/easynlp/modelzoo/models/artist/modeling_artist.py:65: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
      "/home/yubin/EasyNLP-alibaba/easynlp/appzoo/text2image_generation/vqgan.py:471: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  w_ = w_ * (int(c)**(-0.5))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yubin/EasyNLP-alibaba/predict.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=7'>8</a>\u001b[0m example_output \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mmodel(dummy_input1, dummy_input2)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(example_output\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=10'>11</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(predictor\u001b[39m.\u001b[39;49mmodel, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=11'>12</a>\u001b[0m                 (dummy_input1, dummy_input2),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=12'>13</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39moutput/text2image.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=13'>14</a>\u001b[0m                 opset_version\u001b[39m=\u001b[39;49m\u001b[39m11\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=14'>15</a>\u001b[0m                 input_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mtext_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcshape\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=15'>16</a>\u001b[0m                 output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mgen_imgs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=16'>17</a>\u001b[0m                 dynamic_axes\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtext_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=17'>18</a>\u001b[0m                               \u001b[39m\"\u001b[39;49m\u001b[39mcshape\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=18'>19</a>\u001b[0m                               \u001b[39m\"\u001b[39;49m\u001b[39mgen_imgs\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000010vscode-remote?line=19'>20</a>\u001b[0m                 })\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/__init__.py:275\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mExport a model into ONNX format.  This exporter runs your model\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39monce in order to get a trace of its execution to be exported;\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m        than ONNX.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mexport(model, args, f, export_params, verbose, training,\n\u001b[1;32m    276\u001b[0m                     input_names, output_names, aten, export_raw_ir,\n\u001b[1;32m    277\u001b[0m                     operator_export_type, opset_version, _retain_param_name,\n\u001b[1;32m    278\u001b[0m                     do_constant_folding, example_outputs,\n\u001b[1;32m    279\u001b[0m                     strip_doc_string, dynamic_axes, keep_initializers_as_inputs,\n\u001b[1;32m    280\u001b[0m                     custom_opsets, enable_onnx_checker, use_external_data_format)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/utils.py:88\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         operator_export_type \u001b[39m=\u001b[39m OperatorExportTypes\u001b[39m.\u001b[39mONNX\n\u001b[0;32m---> 88\u001b[0m _export(model, args, f, export_params, verbose, training, input_names, output_names,\n\u001b[1;32m     89\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type, opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m     90\u001b[0m         _retain_param_name\u001b[39m=\u001b[39;49m_retain_param_name, do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m     91\u001b[0m         example_outputs\u001b[39m=\u001b[39;49mexample_outputs, strip_doc_string\u001b[39m=\u001b[39;49mstrip_doc_string,\n\u001b[1;32m     92\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes, keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m     93\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets, enable_onnx_checker\u001b[39m=\u001b[39;49menable_onnx_checker,\n\u001b[1;32m     94\u001b[0m         use_external_data_format\u001b[39m=\u001b[39;49muse_external_data_format)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/utils.py:689\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference)\u001b[0m\n\u001b[1;32m    685\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m    686\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m    688\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 689\u001b[0m     _model_to_graph(model, args, verbose, input_names,\n\u001b[1;32m    690\u001b[0m                     output_names, operator_export_type,\n\u001b[1;32m    691\u001b[0m                     example_outputs, _retain_param_name,\n\u001b[1;32m    692\u001b[0m                     val_do_constant_folding,\n\u001b[1;32m    693\u001b[0m                     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m    694\u001b[0m                     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    695\u001b[0m                     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes)\n\u001b[1;32m    697\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m    698\u001b[0m defer_weight_export \u001b[39m=\u001b[39m export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m ExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/utils.py:463\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, example_outputs, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m    458\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args,\n\u001b[1;32m    459\u001b[0m                                                      _retain_param_name)\n\u001b[1;32m    461\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m--> 463\u001b[0m graph \u001b[39m=\u001b[39m _optimize_graph(graph, operator_export_type,\n\u001b[1;32m    464\u001b[0m                         _disable_torch_constant_prop\u001b[39m=\u001b[39;49m_disable_torch_constant_prop,\n\u001b[1;32m    465\u001b[0m                         fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size, params_dict\u001b[39m=\u001b[39;49mparams_dict,\n\u001b[1;32m    466\u001b[0m                         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes, input_names\u001b[39m=\u001b[39;49minput_names,\n\u001b[1;32m    467\u001b[0m                         module\u001b[39m=\u001b[39;49mmodule)\n\u001b[1;32m    468\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msymbolic_helper\u001b[39;00m \u001b[39mimport\u001b[39;00m _onnx_shape_inference\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(model, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction):\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/utils.py:200\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    198\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m dynamic_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m dynamic_axes\n\u001b[1;32m    199\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[0;32m--> 200\u001b[0m graph \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_jit_pass_onnx(graph, operator_export_type)\n\u001b[1;32m    201\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_lint(graph)\n\u001b[1;32m    203\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msymbolic_helper\u001b[39;00m \u001b[39mimport\u001b[39;00m _export_onnx_opset_version\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/__init__.py:311\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m    308\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mselect_model_mode_for_export(model, mode)\n\u001b[0;32m--> 311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_symbolic_function\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    312\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m    313\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39m_run_symbolic_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Export the trained model to ONNX\n",
    "dummy_input1 = torch.tensor([[24326, 22066, 21288, 17484, 23535, 21686, 22520, 21381, 22547, 17420,\n",
    "         21381, 17463, 19406, 17061, 22516, 16384, 16384, 16384, 16384, 16384,\n",
    "         16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384,\n",
    "         16384, 16384]], device='cuda:0') \n",
    "dummy_input2 = torch.tensor([  1, 256,  16,  16])\n",
    "\n",
    "example_output = predictor.model(dummy_input1, dummy_input2)\n",
    "print(example_output.shape)\n",
    "\n",
    "torch.onnx.export(predictor.model, \n",
    "                (dummy_input1, dummy_input2),\n",
    "                \"output/text2image.onnx\", \n",
    "                opset_version=11,\n",
    "                input_names=[\"text_ids\", \"cshape\"],\n",
    "                output_names=[\"gen_imgs\"],\n",
    "                dynamic_axes={\"text_ids\": {0: \"batch_size\"},\n",
    "                              \"cshape\": {0: \"batch_size\"},\n",
    "                              \"gen_imgs\": {0: \"batch_size\"}\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'zshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yubin/EasyNLP-alibaba/predict.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m# Export the trained model to ONNX\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000003vscode-remote?line=3'>4</a>\u001b[0m dummy_input \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m))\u001b[39m.\u001b[39mlong() \u001b[39m# one black and white 28 x 28 picture will be the input to the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.2/home/yubin/EasyNLP-alibaba/predict.ipynb#ch0000003vscode-remote?line=4'>5</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(predictor\u001b[39m.\u001b[39;49mmodel, dummy_input, \u001b[39m\"\u001b[39;49m\u001b[39moutput/text2image.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/__init__.py:275\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mExport a model into ONNX format.  This exporter runs your model\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39monce in order to get a trace of its execution to be exported;\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m        than ONNX.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mexport(model, args, f, export_params, verbose, training,\n\u001b[1;32m    276\u001b[0m                     input_names, output_names, aten, export_raw_ir,\n\u001b[1;32m    277\u001b[0m                     operator_export_type, opset_version, _retain_param_name,\n\u001b[1;32m    278\u001b[0m                     do_constant_folding, example_outputs,\n\u001b[1;32m    279\u001b[0m                     strip_doc_string, dynamic_axes, keep_initializers_as_inputs,\n\u001b[1;32m    280\u001b[0m                     custom_opsets, enable_onnx_checker, use_external_data_format)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/utils.py:88\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         operator_export_type \u001b[39m=\u001b[39m OperatorExportTypes\u001b[39m.\u001b[39mONNX\n\u001b[0;32m---> 88\u001b[0m _export(model, args, f, export_params, verbose, training, input_names, output_names,\n\u001b[1;32m     89\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type, opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m     90\u001b[0m         _retain_param_name\u001b[39m=\u001b[39;49m_retain_param_name, do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m     91\u001b[0m         example_outputs\u001b[39m=\u001b[39;49mexample_outputs, strip_doc_string\u001b[39m=\u001b[39;49mstrip_doc_string,\n\u001b[1;32m     92\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes, keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m     93\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets, enable_onnx_checker\u001b[39m=\u001b[39;49menable_onnx_checker,\n\u001b[1;32m     94\u001b[0m         use_external_data_format\u001b[39m=\u001b[39;49muse_external_data_format)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/utils.py:689\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference)\u001b[0m\n\u001b[1;32m    685\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m    686\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m    688\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 689\u001b[0m     _model_to_graph(model, args, verbose, input_names,\n\u001b[1;32m    690\u001b[0m                     output_names, operator_export_type,\n\u001b[1;32m    691\u001b[0m                     example_outputs, _retain_param_name,\n\u001b[1;32m    692\u001b[0m                     val_do_constant_folding,\n\u001b[1;32m    693\u001b[0m                     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m    694\u001b[0m                     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    695\u001b[0m                     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes)\n\u001b[1;32m    697\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m    698\u001b[0m defer_weight_export \u001b[39m=\u001b[39m export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m ExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/utils.py:458\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, example_outputs, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(example_outputs, (torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m)):\n\u001b[1;32m    456\u001b[0m     example_outputs \u001b[39m=\u001b[39m (example_outputs,)\n\u001b[0;32m--> 458\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args,\n\u001b[1;32m    459\u001b[0m                                                      _retain_param_name)\n\u001b[1;32m    461\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m    463\u001b[0m graph \u001b[39m=\u001b[39m _optimize_graph(graph, operator_export_type,\n\u001b[1;32m    464\u001b[0m                         _disable_torch_constant_prop\u001b[39m=\u001b[39m_disable_torch_constant_prop,\n\u001b[1;32m    465\u001b[0m                         fixed_batch_size\u001b[39m=\u001b[39mfixed_batch_size, params_dict\u001b[39m=\u001b[39mparams_dict,\n\u001b[1;32m    466\u001b[0m                         dynamic_axes\u001b[39m=\u001b[39mdynamic_axes, input_names\u001b[39m=\u001b[39minput_names,\n\u001b[1;32m    467\u001b[0m                         module\u001b[39m=\u001b[39mmodule)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/utils.py:422\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args, _retain_param_name)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m     graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m    423\u001b[0m     state_dict \u001b[39m=\u001b[39m _unique_state_dict(model)\n\u001b[1;32m    424\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(state_dict\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/onnx/utils.py:373\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_trace_and_get_graph_from_model\u001b[39m(model, args):\n\u001b[1;32m    367\u001b[0m \n\u001b[1;32m    368\u001b[0m     \u001b[39m# A basic sanity check: make sure the state_dict keys are the same\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     \u001b[39m# before and after running the model.  Fail fast!\u001b[39;00m\n\u001b[1;32m    370\u001b[0m     orig_state_dict_keys \u001b[39m=\u001b[39m _unique_state_dict(model)\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m    372\u001b[0m     trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 373\u001b[0m         torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(model, args, strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    374\u001b[0m     warn_on_static_input_change(inputs_states)\n\u001b[1;32m    376\u001b[0m     \u001b[39mif\u001b[39;00m orig_state_dict_keys \u001b[39m!=\u001b[39m _unique_state_dict(model)\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/jit/_trace.py:1160\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1159\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1160\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1161\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    128\u001b[0m     wrapper,\n\u001b[1;32m    129\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    130\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1039\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1039\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1040\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'zshape'"
     ]
    }
   ],
   "source": [
    "# from torch.autograd import Variable\n",
    "# import torch\n",
    "# # Export the trained model to ONNX\n",
    "# dummy_input = Variable(torch.randn(1, 32)).long() # one black and white 28 x 28 picture will be the input to the model\n",
    "# torch.onnx.export(predictor.model, dummy_input, \"output/text2image.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59c0781f99bb873beca84764973919837529f603c5934888e3f18bbeeda74045"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
